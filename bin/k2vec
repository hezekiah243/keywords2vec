#!/usr/bin/env python
import os
import sys

from argparse import ArgumentParser

from tornado.wsgi import WSGIContainer
from tornado.httpserver import HTTPServer
from tornado.ioloop import IOLoop

from keywords2vec import tokenize_file, train_model
from keywords2vec.server import prepare_server


def main():
    parser = ArgumentParser()
    subparsers = parser.add_subparsers(dest='subcommand')
    subparsers.required = True

    parser_train = subparsers.add_parser('train', help='train a model using a input file')

    parser_train.add_argument(
        "-i", "--input-file", dest="input_filename",
        required=True, help="the input text file", metavar="INPUT_FILE"
    )
    parser_train.add_argument(
        "-o", "--output-directory", dest="output_directory",
        required=True, help="the target directory for the outputs",
        metavar="DIRECTORY"
    )
    parser_train.add_argument(
        "-s", "--sample", dest="sample_size",
        required=False,
        help="if you wanted to process a sample of the lines. By default -1 (all)",
        metavar="SIZE", default="-1", type=int
    )
    parser_train.add_argument(
        "-l", "--lines-chunks", dest="lines_chunks",
        required=False, help="size of the lines chunks, to use as progress update (-1 auto)",
        metavar="LINES_CHUNKS", default="-1", type=int
    )
    parser_train.add_argument(
        "--word2vec_size", dest="word2vec_size",
        required=False,
        help="Word2vec vector size",
        metavar="WORD2VEC_SIZE", default="300", type=int
    )
    parser_train.add_argument(
        "--lang", dest="lang",
        required=False,
        help="Language",
        metavar="lang", default="en", type=str
    )
    parser_train.add_argument(
        "--word2vec_window", dest="word2vec_window",
        required=False,
        help="Word2vec window size",
        metavar="WORD2VEC_WINDOW", default="3", type=int
    )
    parser_train.add_argument(
        "--word2vec_count", dest="word2vec_count",
        required=False,
        help="Word2vec min keyword count",
        metavar="WORD2VEC_MIN_COUNT", default="3", type=int
    )
    parser_train.add_argument(
        "--word2vec_epochs", dest="word2vec_epochs",
        required=False,
        help="Word2vec epochs number",
        metavar="WORD2VEC_EPOCHS", default="10", type=int
    )
    parser_train.add_argument(
        "--workers", dest="workers",
        required=False,
        help="Total numbers of CPU workers",
        metavar="CPU_WORKERS", default="-1", type=int
    )
    parser_train.add_argument(
        "--tokenize-only", dest="tokenize_only",
        required=False,
        help="If you wanted to only tokenize the text",
        action='store_true'
    )


    parser_serve = subparsers.add_parser('serve', help='serve a trained model')
    parser_serve.add_argument(
        "--model-path", dest="model_path",
        required=True,
        help="The path to the model"
    )
    parser_serve.add_argument(
        "--index-path", dest="index_path",
        required=True,
        help="The path to the indexed"
    )

    try:
        args = parser.parse_args()
    except TypeError as e:
        parser.print_help(sys.stderr)
        return

    if args.subcommand == "serve":
        app = prepare_server(args.model_path, args.index_path)
        http_server = HTTPServer(WSGIContainer(app))
        http_server.listen(5000)
        IOLoop.instance().start()
    elif args.subcommand == "train":
        if not os.path.exists(args.output_directory):
            os.makedirs(args.output_directory)

        tokenized_file = os.path.join(args.output_directory, "tokenized.txt")

        output_path = tokenize_file(args.input_filename, tokenized_file)
        if output_path:
            model = train_model(tokenized_file)
            model_path = os.path.join(args.output_directory, "model.bin")
            model.save_model(model_path)


if __name__ == '__main__':
    main()


