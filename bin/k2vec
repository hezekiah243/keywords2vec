#!/usr/bin/env python
import os

from argparse import ArgumentParser

from keywords2vec import tokenize_file, train_model


def main():
    parser = ArgumentParser()
    parser.add_argument(
        "-i", "--input-file", dest="input_filename",
        required=True, help="the input text file", metavar="INPUT_FILE"
    )
    parser.add_argument(
        "-o", "--output-directory", dest="output_directory",
        required=True, help="the target directory for the outputs",
        metavar="DIRECTORY"
    )
    parser.add_argument(
        "-s", "--sample", dest="sample_size",
        required=False,
        help="if you wanted to process a sample of the lines. By default -1 (all)",
        metavar="SIZE", default="-1", type=int
    )
    parser.add_argument(
        "-l", "--lines-chunks", dest="lines_chunks",
        required=False, help="size of the lines chunks, to use as progress update (-1 auto)",
        metavar="LINES_CHUNKS", default="-1", type=int
    )
    parser.add_argument(
        "--word2vec_size", dest="word2vec_size",
        required=False,
        help="Word2vec vector size",
        metavar="WORD2VEC_SIZE", default="300", type=int
    )
    parser.add_argument(
        "--lang", dest="lang",
        required=False,
        help="Language",
        metavar="lang", default="en", type=str
    )
    parser.add_argument(
        "--word2vec_window", dest="word2vec_window",
        required=False,
        help="Word2vec window size",
        metavar="WORD2VEC_WINDOW", default="3", type=int
    )
    parser.add_argument(
        "--word2vec_count", dest="word2vec_count",
        required=False,
        help="Word2vec min keyword count",
        metavar="WORD2VEC_MIN_COUNT", default="3", type=int
    )
    parser.add_argument(
        "--word2vec_epochs", dest="word2vec_epochs",
        required=False,
        help="Word2vec epochs number",
        metavar="WORD2VEC_EPOCHS", default="10", type=int
    )
    parser.add_argument(
        "--workers", dest="workers",
        required=False,
        help="Total numbers of CPU workers",
        metavar="CPU_WORKERS", default="-1", type=int
    )
    parser.add_argument(
        "--tokenize-only", dest="tokenize_only",
        required=False,
        help="If you wanted to only tokenize the text",
        action='store_true'
    )

    args = parser.parse_args()

    if not os.path.exists(args.output_directory):
        os.makedirs(args.output_directory)

    tokenized_file = os.path.join(args.output_directory, "tokenized.txt")

    tokenize_file(args.input_filename, tokenized_file)
    model = train_model(tokenized_file)
    model_path = os.path.join(args.output_directory, "model.bin")
    model.save_model(model_path)


if __name__ == '__main__':
    main()
